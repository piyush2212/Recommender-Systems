{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching Movie Lens Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ml-100k.zip -d sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining various datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "dir = 'sample_data/ml-100k'\n",
    "col_names = ['user id', 'item id', 'rating', 'timestamp']\n",
    "data = pd.read_csv(os.path.join(dir, 'u.data'), delimiter='\\t', names=col_names, header=None)\n",
    "data['timestamp'] = data['timestamp'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "\n",
    "with open(os.path.join(dir, 'u.item'), encoding = \"ISO-8859-1\") as f:\n",
    "  movie = pd.read_csv(f, delimiter='|', header=None)\n",
    "\n",
    "movie.columns = ['item id', 'title' ,'release date','video release date', 'IMDb URL', 'unknown', 'Action', 'Adventure',\n",
    " 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    " 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "with open(os.path.join(dir, 'u.user'), encoding = \"ISO-8859-1\") as f:\n",
    "  user = pd.read_csv(f, delimiter='|', header=None)\n",
    "\n",
    "user.columns = ['user id', 'age', 'gender', 'occupation', 'zip code']\n",
    "\n",
    "ratings = data.merge(movie[['item id', 'title']], on='item id')\n",
    "\n",
    "ratings['like'] = ratings['rating'] > 3\n",
    "\n",
    "ratings.sort_values(by=['user id'],inplace=True)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Df in test and train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "train_size = int(len(ratings)*train_ratio)\n",
    "ratings_train = ratings.sample(train_size, random_state=42)\n",
    "ratings_test = ratings[~ratings.index.isin(ratings_train.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "n_users = ratings_train['user id'].max()\n",
    "n_item = ratings_train['item id'].max()\n",
    "ratings_train_pos = ratings_train[ratings_train['like']]\n",
    "ratings_test_pos = ratings_test[ratings_test['like']]\n",
    "\n",
    "\n",
    "row=ratings_train_pos['user id'].values - 1\n",
    "col=ratings_train_pos['item id'].values - 1\n",
    "data=np.ones(len(ratings_train_pos))\n",
    "user_item_data = csr_matrix((data, (row, col)), shape=(n_users, n_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import implicit\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors=50, random_state=42)\n",
    "\n",
    "model.fit(user_item_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to recommend items based on above matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import dcg_score, ndcg_score\n",
    "\n",
    "def precision_k(actuals, recs, k=5):\n",
    "  return len(set(recs[0:k]).intersection(set(actuals)))/k\n",
    "\n",
    "def recall_k(actuals, recs, k=5):\n",
    "  return len(set(recs[0:k]).intersection(set(actuals)))/len(actuals)\n",
    "\n",
    "def dcg_k(actuals, recs, k=5):\n",
    "  relevance = np.array([[float(i in actuals) for i in recs[0:k]]])\n",
    "  score = k - np.arange(k)\n",
    "  return dcg_score(relevance, score.reshape(1,-1), k=k)\n",
    "\n",
    "def ndcg_k(actuals, recs, k=5):\n",
    "  relevance = np.array([[float(i in actuals) for i in recs[0:k]]])\n",
    "  score = k - np.arange(k)\n",
    "  return ndcg_score(relevance, score.reshape(1,-1), k=k)\n",
    "\n",
    "def recall_stage(model, user_id, user_item_data, ratings_train,N_recall ):\n",
    "  filter_items = ratings_train[ratings_train['user id']==user_id]['item id'].values\n",
    "  filter_items = filter_items - 1\n",
    "  user_id = user_id - 1\n",
    "\n",
    "  recs, scores = model.recommend(user_id, \n",
    "                                 user_item_data[user_id], \n",
    "                                 filter_items=filter_items,\n",
    "                                 N=N_recall)\n",
    "  recs = recs.flatten() + 1\n",
    "  return recs,scores\n",
    "\n",
    "def evaluate(user_id, ratings_test_pos, recs, k=5):\n",
    "  actuals = ratings_test_pos[ratings_test_pos['user id']==user_id]['item id'].values\n",
    "  return precision_k(actuals, recs, k), recall_k(actuals, recs, k), dcg_k(actuals, recs, k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluated Metrics for a sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs,scores = recall_stage(model,5,user_item_data,ratings_train,5)\n",
    "evaluate(5,ratings_test_pos,recs,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Recommmendation based on Matrix Factorization for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie.iloc[recs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking of Recommmendatios above by a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"API Key successfully retrieved!\")\n",
    "else:\n",
    "    print(\"API Key not found. Please set the environment variable.\")\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(api_key = OPENAI_API_KEY,temperature=0.0, model=llm_model)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"The person has a list of liked movies: {movies_liked}. \\\n",
    "The person has a list of disliked movies: {movies_disliked}. \\\n",
    "Tell me if this person likes each of the candidate movies: {movies_candidates}.\\\n",
    "Return a list of boolean values and explain why the person likes or dislikes.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a list of JSON object formatted to look like:\n",
    "{{\n",
    "    \"title\": string \\ the name of the movie in candidate movies\n",
    "    \"like\": boolean \\ true or false\n",
    "    \"explanation\": string \\ explain why the person likes or dislikes the candidate movie\n",
    "}}\n",
    "\n",
    "\n",
    "REMEMBER: Each boolean and explanation for each element in candidate movies.\n",
    "REMEMBER: The explanation must relate to the person's liked and disliked movies.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_new = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an AI assistant. Answer the following question concisely: {question}\"\"\"\n",
    ")\n",
    "\n",
    "# Create the LLM chain with the prompt and OpenAI model\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = {\n",
    "    \"question\": \"What is the capital of France?\"\n",
    "}\n",
    "chain = LLMChain(llm=llm, prompt=prompt_new)\n",
    "# Run the chain with the sample input\n",
    "response = chain.run(sample_input)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_stage(chain, user_id, ratings_train, pre_recs, movie, batch_size=10):\n",
    "\n",
    "  few_shot = ratings_train[(ratings_train['user id']==user_id)]\n",
    "  if len(few_shot) >= 20:\n",
    "    few_shot = few_shot.sample(20, random_state=42)\n",
    "  recall_recs = movie.set_index('item id').loc[pre_recs].reset_index()\n",
    "\n",
    "  movies_liked = ','.join(few_shot[few_shot['like']]['title'].values.tolist())\n",
    "  movies_disliked = ','.join(few_shot[~few_shot['like']]['title'].values.tolist())\n",
    "\n",
    "  n_batch = int(np.ceil(len(recall_recs)/batch_size))\n",
    "  candidates = recall_recs[['item id', 'title']]\n",
    "  result_json = []\n",
    "\n",
    "  for i in range(n_batch):\n",
    "    candidates_batch = candidates.iloc[i*batch_size: (i+1)*batch_size]\n",
    "    movies_candidates = ','.join(candidates_batch['title'].values.tolist())\n",
    "    result = chain.run(movies_liked=movies_liked, movies_disliked=movies_disliked, movies_candidates=movies_candidates)\n",
    "    result_list = result.replace('\\n', '').replace('},', '}\\n,').split('\\n,')\n",
    "    result_json_batch = [json.loads(i) for i in result_list]\n",
    "    result_json = result_json + result_json_batch\n",
    "\n",
    "  result_rank = pd.DataFrame.from_dict(result_json)\n",
    "  result_rank['item id'] = recall_recs['item id'].values\n",
    "  result_rank = pd.concat([result_rank[result_rank['like']], result_rank[~result_rank['like']]])\n",
    "\n",
    "  return result_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_result = ranking_stage(chain, 5, ratings_train, recs, movie)\n",
    "rank_recs = rank_result['item id'].values\n",
    "\n",
    "p, r, ndcg = evaluate(5, ratings_test_pos, rank_recs, k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
